<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | t0m's very occasional blog]]></title>
  <link href="http://bobtfish.github.io/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://bobtfish.github.io/"/>
  <updated>2015-04-12T00:05:28+01:00</updated>
  <id>http://bobtfish.github.io/</id>
  <author>
    <name><![CDATA[Tomas Doran]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DIY Scalable PAAS with Terraform]]></title>
    <link href="http://bobtfish.github.io/blog/2015/04/06/diy-scalable-paas-with-terraform/"/>
    <updated>2015-04-06T23:45:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2015/04/06/diy-scalable-paas-with-terraform</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been ranting about <a href="https://www.terraform.io/">Terraform</a> <a href="/blog/2015/03/29/terraform-from-the-ground-up/">a</a> <a href="/blog/2015/04/03/terraform-0-dot-4-0/">lot</a> recently, and I&rsquo;ve shown some pretty
neat examples of building a VPC &ndash; however before now I haven&rsquo;t got anything <em>actually</em>
useful running.</p>

<p>In this post, we&rsquo;re gonna change that, and launch ourselves a publicly
available and scalable PAAS (Platform As A Service), built around Apache <a href="http://mesos.apache.org/">Mesos</a> and <a href="https://github.com/mesosphere/marathon">Marathon</a>
with <a href="http://nginx.org/">nginx</a> and Amazon&rsquo;s <a href="http://aws.amazon.com/elasticloadbalancing/">ELB</a> for load balancing and <a href="http://aws.amazon.com/route53/">Route53</a> for DNS.</p>

<!-- more -->


<h2>Getting setup</h2>

<p>You&rsquo;ll need an AWS account, a Ubuntu machine and a domain you control (which doesn&rsquo;t have to be in AWS) for this demo.
My example domain is <em>notanisp.net</em>, and so the subdomain I&rsquo;m going to use in all the examples is <em>mesos.notanisp.net</em></p>

<p>You&rsquo;ll also need the <a href="http://aws.amazon.com/cli/">AWS cli</a> and an <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-config-files">~/.aws/credentials</a>
file with a <em>[demo]</em> section in it.</p>

<p>In your AWS account, you need to go into the IAM menus, and create a &lsquo;<a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/roles-usingrole-instanceprofile.html">Role</a>&rsquo;,
which will be given to the machines we launch, to <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ExamplePolicies_EC2.html">allow them to find out other instances</a>.</p>

<p>Your policy should be named <em>describe-instances</em> and the body of the policy should contain an inline policy:</p>

<pre><code>{
   "Version": "2012-10-17",
   "Statement": [{
      "Effect": "Allow",
      "Action": "ec2:Describe*",
      "Resource": "*"
    }
   ]
}
</code></pre>

<p>If you&rsquo;re feeling lazy, then you can just create the role with the policy of AmazonEC2ReadOnlyAccess.</p>

<h2>Getting terraform</h2>

<p>The easiest way to get my patched version of terraform is to build it in vagrant:</p>

<p>```</p>

<pre><code>git clone https://github.com/bobtfish/terraform.git
cd terraform
vagrant up
vagrant ssh
sudo apt-get install -y ruby ruby-json python-pip
sudo pip install awscli
export PATH=$PATH:/opt/go/bin:/opt/gopath/bin
echo 'export PATH=$PATH:/opt/go/bin:/opt/gopath/bin' &gt;&gt; ~/.bashrc
mkdir -p /opt/gopath/src/github.com/hashicorp
cp -r /vagrant /opt/gopath/src/github.com/hashicorp/terraform
cd /opt/gopath/src/github.com/hashicorp/terraform
make updatedeps
make dev
cd ~/
git clone https://github.com/bobtfish/terraform-example-mesos-cluster
mkdir ~/.aws
vi ~/.aws/credentials
</code></pre>

<p>```</p>

<p>Adding something like this:</p>

<pre><code>[demo]
aws_access_key_id=AAAAAAAA
aws_secret_access_key=BBBBBBBBBBBBBBBBB
</code></pre>

<p>Or if you prefer (and you trust me!), <a href="http://notanisp-packages.s3-website-eu-west-1.amazonaws.com/dists/precise/main/binary-amd64/terraform_0.4.0-bobtfish0_amd64.deb">here&rsquo;s a .deb</a>
you should be able to just install:</p>

<h2>Building your cluster</h2>

<p>Once you&rsquo;re all setup, you can get started building your cluster:</p>

<p><code>
cd terraform-example-mesos-cluster
make
</code></p>

<p>This generates an ssh key (stored in <em>id_rsa</em>) to be able to ssh into your instances,
and captures your IP (stored in <em>admin_iprange.txt</em>) to allow access to the Mesos and
Marathon admin interfaces later. Note: You can edit the admin_iprange.txt file to
allow more IPs access, but it is <em>not</em> recommended to change this to 0.0.0.0/0,
as otherwise random people on the internet will be able to run jobs on your Mesos
cluster!</p>

<p>Lets go ahead and build out the cluster:</p>

<p><code>
cd eu-central-1
vi terraform.tfvars # Adjust the 'domain' value to a subdomain
make # This pulls down all the terraform modules you need, and builds a map of which AZs your account can see.
terraform apply
</code></p>

<p>The last step will take a while, go grab a coffee.</p>

<p>Eventually, terraform will finish, with output like this:</p>

<p>```
Apply complete! Resources: 35 added, 0 changed, 0 destroyed.</p>

<p>The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the <code>terraform show</code> command.</p>

<p>State path: terraform.tfstate</p>

<p>Outputs:</p>

<p>  marathon_api  = <a href="http://marathon.admin.mesos.notanisp.net">http://marathon.admin.mesos.notanisp.net</a>
  nat_public_ip = 52.28.37.228
```</p>

<p>Once it&rsquo;s finished, go into the route 53 control panel, and you should see your subdomain zone.
Go and grab its NS records:</p>

<p><img src="https://raw.githubusercontent.com/bobtfish/terraform-example-mesos-cluster/master/route53.png" alt="Route53 console" /></p>

<p>and put those into whatever you use to manage your main domain as a delegated record.</p>

<h2>Gettins admin pages and launching an app</h2>

<p>After DNS catches up, you should be able to resolve:</p>

<ul>
<li>mesos.admin.mesos.notanisp.net</li>
<li>marathon.admin.mesos.notanisp.net</li>
<li>www.mesos.notanisp.net</li>
</ul>


<p>Hit <a href="http://marathon.admin.mesos.notanisp.net">http://marathon.admin.mesos.notanisp.net</a> with your browser, and you should be able to see the
Marathon admin screen. (And you can see the Mesos admin screen at <a href="http://mesos.admin.mesos.notanisp.net">http://mesos.admin.mesos.notanisp.net</a>)</p>

<p>You can now <a href="https://mesosphere.com/docs/tutorials/run-services-with-marathon">launch an app</a>!</p>

<p>I&rsquo;ve automated the launching of an example application, so you can just say:</p>

<pre><code>make deploywww
</code></pre>

<p>This will deploy <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/marathon_www.json">the hello world app</a>
 (from the mesosphere tutorial linked above) named &lsquo;/www&rsquo;
into marathon:</p>

<p><img src="/images/marathon_deploy.png" alt="App deploying" /></p>

<p>and after a couple of mins, you should be able to access it from <a href="http://www.mesos.notanisp.net">http://www.mesos.notanisp.net</a></p>

<p>Any additional apps you launch in Marathon with names like /someapp will be automatically bound
to a vhost on the load balancer. Pretty neat, eh?</p>

<h2>What we&rsquo;re building behind the scenes.</h2>

<p>We <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/vpc.tf">ask for a VPC to be built</a>
(with the <a href="https://github.com/bobtfish/terraform-vpc-nat">module I created</a> in <a href="http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up/">a previous post</a>).</p>

<p>This gives us &lsquo;private&rsquo; and &lsquo;public&rsquo; subnets, and a <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html">NAT instance</a>.</p>

<p>On top of that, we build out a <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/mesos.tf">mesos cluster</a></p>

<p>This comprises a number of pieces, first of all <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/mesos_master/master.conf">masters</a>, which
run <a href="https://zookeeper.apache.org/">zookeeper</a> in addition to Marathon and Mesos. We run 3 of these by default, so one can fail without
affecting the operation of the cluster. Also there are <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/mesos_slave/slave.conf">slaves</a>
which actually run the Mesos workers and excute applications. We launch 5 of these, and if one fails then any tasks it was running
will be re-launched on the remaining slaves (assuming they have enough resources to do so!).</p>

<p>Then we deploy 2 <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/lb/lb.conf">load balancers</a> which discover the running applications from the marathon
API and configure nginx, with a publicly accessible <a href="http://aws.amazon.com/elasticloadbalancing/">ELB</a> <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/elb/main.tf">in front of them</a>
 to provide HA, in the event one of the load balancer machines fails.</p>

<p>We push this <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/dns/main.tf">into DNS</a>, along with records for the <em>adminlb</em> machine.
<a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/elb/main.tf">This machine</a> is what proxies the mesos and marathon admin pages (and access to it
is locked down to your <em>admin_iprange.txt</em>)</p>

<h2>Redundancy and scaling</h2>

<p>The cluster should be redundant to a load balancer or a mesos master failing. If the instance which fails can&rsquo;t be brought back
online, it should be possible to just terminate the instance and have terraform re-create it.</p>

<p>Also, you can eaisly adjust the number of slaves dynamically (just by <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/mesos.tf#L6">changing the variable</a>
 and running terraform), or the instance types employed (by rebuilding the cluster) if you&rsquo;d like to run more tasks, serve real
load from the cluster, or scale whilst already serving real load!</p>

<p>Marathon apps which have already been deployed can be scaled up (to more instances) just by using the Marathon API with a web browser &ndash; I encourage you to try this, scaling
the example app up to 5 instances, or down to 1 instance and then killing 4/5 of the slaves.</p>

<h2>TODOs</h2>

<ul>
<li><p>Whilst the cluster we build is redundant, currently all the machines are allocated in a single availability zone.
I&rsquo;ll be investigating if I can get a way of generically deploying the cluster across two (or three if available) availability zones.</p></li>
<li><p>Currently the machine&rsquo;s Internet access goes through a single NAT instance with no failover &ndash; this needs rectifying.</p></li>
<li><p>The machines have no configuration management (no puppet/chef), which means that making any changes to them (or getting
any security updates) involves rebuilding the instances.</p></li>
</ul>


<h2>Conclusion</h2>

<p>Hopefully this post has shown that Terraform is already grown up enough to be used to compose real, production ready infrastructures.</p>

<p>The example shown here <em>is not</em> something I&rsquo;d run as a production system, and there are a number of significant warts
in the code (like having to pack and unpack lists into comma seperated strings), but there&rsquo;s enough of the essential complexity
represented here to be close to &lsquo;real&rsquo; applications and deployment, and I&rsquo;m still having a great time exploring Terraform&rsquo;s
rapidly growing capabilities, and producing reuseable modules.</p>

<p>Whilst I don&rsquo;t think that anything but my most basic level of modules (e.g. AMI lookups) are likely to be reused verbatim,
I hope this example shows that it&rsquo;s possible to use terraform as a turnkey component in building repeatable
and composable infrastructures.</p>

<h2>Credits</h2>

<p>Container Solutions have <a href="http://container-solutions.com/2015/04/how-to-set-up-mesos-on-google-cloud-with-terraform/">an excellent blog post</a>
on making a Mesos cluster on <a href="https://cloud.google.com/compute/">GCE</a> &ndash; I stole it, converted it to AWS and added the service discovery/load balancing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Terraform 0.4.0]]></title>
    <link href="http://bobtfish.github.io/blog/2015/04/03/terraform-0-dot-4-0/"/>
    <updated>2015-04-03T00:48:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2015/04/03/terraform-0-dot-4-0</id>
    <content type="html"><![CDATA[<p><a href="https://hashicorp.com/blog/terraform-0-4.html">Terraform 0.4.0 is out</a>, with the <a href="https://github.com/hashicorp/terraform/pull/1185">remote modules</a> feature!
As per <a href="http://localhost:4000/blog/2015/03/29/terraform-from-the-ground-up/">my last post</a>,
I&rsquo;ve been playing around with this already, and I thought it was worth showing
off my simple example of this feature actually being used.</p>

<p>It&rsquo;s wrapped up in a way that you can fork my 2 repositories and play with it
yourself, and I encourage <a href="https://github.com/bobtfish/terraform-example-vpc-infra/blob/master/eucentral1-demo/terraform.tfvars#L3">you to do so</a></p>

<!-- more -->


<p>If you take <a href="https://github.com/bobtfish/terraform-example-vpc/tree/master/eucentral1-demo">the example VPC</a>
from my last post (with the state file committed), you can now pull in the data in another terraform
repository <a href="https://github.com/bobtfish/terraform-example-vpc-infra/blob/master/eucentral1-demo/vpc.tf">like this</a>
and then use the outputs it defines <a href="https://github.com/bobtfish/terraform-example-vpc-infra/blob/master/eucentral1-demo/kubernates.tf#L8">like this</a>.</p>

<p>This is pretty neat, and it&rsquo;ll be super cool to allow multiple teams to work on different layers
of the infrastructure, using the <a href="https://www.terraform.io/docs/commands/remote.html">consul state store</a> and <a href="https://www.consul.io/docs/internals/acl.html">ACLs</a>,
more news on that once I&rsquo;ve had chance to play with it.</p>

<p>Unfortunately, the credentials file feature didn&rsquo;t make 0.4.0, and I <a href="https://github.com/bobtfish/terraform-aws-coreos-kubernates-cluster/blob/master/nodes.tf#L28">do math that isn&rsquo;t possible in master</a> now, so you&rsquo;ll still need to use my fork if you want to use my examples verbatim.</p>

<p>I&rsquo;ve written a bunch of modules to power my examples though, most of which work on unpatched terraform, and I thought that I&rsquo;d
list them (in most to least reuseable order) so that you don&rsquo;t have to dig through all the code or my github to find them :)</p>

<ul>
<li><a href="https://github.com/bobtfish/terraform-amitype">terraform-amitype</a> &ndash; Match an AMI type (e.g. m3.xlarge) to its virtualization type (e.g. hvm)</li>
<li><a href="https://github.com/bobtfish/terraform-azs">terraform-azs</a> &ndash; Work out which azs each of your accounts has access to for easy lookup/templating across accounts.</li>
<li><a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami">tf_aws_ubuntu_ami</a> &ndash; Look up Ubuntu <a href="http://cloud-images.ubuntu.com/locator/ec2/">AMIs</a></li>
<li><a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami">tf_aws_coreos_ami</a> &ndash; Look up the most recent <a href="https://coreos.com/">CoreOS</a> <a href="https://coreos.com/docs/running-coreos/cloud-providers/ec2/">AMI</a></li>
<li><a href="https://github.com/bobtfish/terraform-vpc">terraform-vpc</a> &ndash; Build out an initial VPC with 2 AZs</li>
<li><a href="https://github.com/bobtfish/terraform-vpc-nat">terraform-vpc-nat</a> &ndash; Build an initial NAT instance out on top of a VPC made with terraform-vpc</li>
<li><a href="https://github.com/bobtfish/terraform-aws-coreos-kubernates-cluster">terraform-aws-coreos-kubernates-cluster</a> &ndash; Kubernates cluster on CoreOS (n.b. not yet working out the box, needs patched terraform)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Terraform from the ground up]]></title>
    <link href="http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up/"/>
    <updated>2015-03-29T21:18:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been playing around with <a href="https://www.terraform.io/">Terraform</a> a bunch recently, and I&rsquo;m pretty <a href="https://twitter.com/bobtfish/status/581905067948797952">excited</a> about <a href="https://github.com/hashicorp/terraform/blob/master/CHANGELOG.md#040-unreleased">0.4.0</a>.</p>

<p>However at the moment, the examples leave quite a lot to be desired. So for my own learning and entertainment, I&rsquo;ve created a set of example Terraform modules,
and a simple example that you should be able to clone and run.</p>

<!-- more -->


<h2>What&rsquo;s exciting in Terraform</h2>

<p>One of the <a href="https://github.com/hashicorp/terraform/pull/1319">pull requests</a> <a href="https://github.com/hashicorp/terraform/pull/1080">I&rsquo;ve been watching</a> <a href="around%20ASGs">https://github.com/hashicorp/terraform/pull/1076</a> / etc were merged recently, <a href="https://github.com/hashicorp/terraform/issues/932">fixing Tags on ASGs</a> which I use at work for the <a href="https://docs.puppetlabs.com/guides/external_nodes.html">puppet ENC</a>, and the <a href="https://github.com/hashicorp/terraform/pull/1049">support for ~/.aws/credentials</a> files is almost ready.</p>

<p>In fact, after chatting with folks at <a href="http://www.scalesummit.org/">scalesummit</a> on Friday, I was so excited that I wanted to pull some of the new features into
<a href="https://github.com/bobtfish/terraform">my fork</a> and play with them.</p>

<p>One of the most exciting strategic features to me is what&rsquo;s been called &lsquo;<a href="https://github.com/hashicorp/terraform/pull/1185">remote modules</a>&rsquo;,
which when it&rsquo;s merged will allow you to consume external Terraform state (in a read-only way) from other Terraform repositories.</p>

<p>This is <em>almost exactly</em> one of the features I&rsquo;d sketched out as something Terraform needed to allow it to scale as a tool for larger organisations.</p>

<p>At my day job, we have multiple teams managing different parts of the infrastructure &ndash; for example one team is responsible for VPCs
and DNS/puppet masters etc, whilst another team is responsible for kafka/zookeeper clusters, and several other teams all having
independently managed Elasticsearch clusters.</p>

<p>Ergo the ability to &lsquo;publish&rsquo; state between teams and thus allowing different teams to share the basics like VPCs and subnets)
whilst having their own configs is essential for the variety of workflows and machine lifecycles that I need to support.</p>

<p>I haven&rsquo;t yet tested out this functionality, as I <a href="http://www.globalnerdy.com/wordpress/wp-content/uploads/2012/09/yak-shaving.jpg">got distracted</a>
making some example &lsquo;base infrastructure&rsquo; modules that I wanted to consume data from.</p>

<h2>Unreleased software note!</h2>

<p>You need to use <a href="https://github.com/bobtfish/terraform">my fork</a> of Terraform for the examples below to work,
but I expect them to work without any significant changes in 0.4.0</p>

<h2>Terraform modules</h2>

<p>I&rsquo;ve scratched my head about how to do Terraform modules (which need to lookup external data) for a while, and I&rsquo;ve come up with a pattern that I don&rsquo;t hate.</p>

<p>I&rsquo;ve also written some public modules that are on github, which you can look at and criticise. (Sorry in advance for the terrible ruby)</p>

<p>Whilst I haven&rsquo;t yet used the feature I merged, the state from the example in this post is a semi-realistic example VPC
that I&rsquo;ll be able to use for further testing.</p>

<p>To give you some insight into how I&rsquo;ve put things together, I&rsquo;m going to dive into
each of the modules and explain a little about them, before
showing how I wrap them up together with the actual code/config to launch a VPC.</p>

<h3>Looking up AMIs</h3>

<p>Lets think about how we should lookup a Ubuntu AMI using <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami">a module</a>.</p>

<p>There&rsquo;s <a href="http://cloud-images.ubuntu.com/locator/ec2">a giant table</a> of AMIs available on the web, and the data for it is <a href="http://cloud-images.ubuntu.com/locator/ec2/releasesTable">almost</a>, <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/getvariables.rb#L11">but not quite</a> JSON you can parse.</p>

<p>So I wrote a getvariables.rb script, a <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/Makefile">Makefile</a> which generates <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/variables.tf.json">variables.tf.json</a> &ndash; end result, we have a giant hash table of all the AMIs for Ubuntu in all of the regions.</p>

<p>We can then use a combination of variables and the <a href="https://github.com/bobtfish/terraform/blob/master/website/source/docs/configuration/interpolation.html.md#user-content-built-in-functions">lookup + format functions</a> in <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/main.tf">the main.tf</a> to output the desired AMI.</p>

<p>Then we just use the module, supplying it the params it needs, and we get the right AMI.</p>

<p>```
module &ldquo;ami&rdquo; {
  source = &ldquo;github.com/terraform-community-modules/tf_aws_ubuntu_ami&rdquo;
  region = &ldquo;eu-central-1&rdquo;
  distribution = &ldquo;trusty&rdquo;
  architecture = &ldquo;amd64&rdquo;
  virttype = &ldquo;hvm&rdquo;
  storagetype = &ldquo;instance-store&rdquo;
}</p>

<p>resource &ldquo;aws_instance&rdquo; &ldquo;web&rdquo; {
  ami = &ldquo;${module.ami.ami_id}&rdquo;
  instance_type = &ldquo;m3.8xlarge&rdquo;
  &hellip;
}
```</p>

<h3>What availability zones?</h3>

<p>Next example &ndash; in AWS, which availability zones you&rsquo;re given access to depends on your account.</p>

<p>Therefore, to provide a generic template that anyone can use to launch a VPC (in any region), we need to be able
to detect which regions and availability zones a user has access to.</p>

<p>Using almost the same pattern, and <a href="http://aws.amazon.com/cli/">the aws cli tool</a>, I&rsquo;ve produced another module
which will read your <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-config-files">~/.aws/credentials file</a> to
pull in a list of all your available availability zones.</p>

<p>Due to the way Terraform currently handles things, <a href="https://github.com/bobtfish/terraform-azs">this module</a> just exports three variables, a primary, secondary, and (where available)
tertiary availability zone (ordered by alphabetical sort).</p>

<p>```
module &ldquo;az&rdquo; {
  source = &ldquo;github.com/terraform-community-modules/tf_aws_availability_zones&rdquo;
  region = &ldquo;eu-central-1&rdquo;
  account = &ldquo;demo&rdquo;
}</p>

<p>resource &ldquo;aws_subnet&rdquo; &ldquo;primary-front&rdquo; {
  availability_zone = &ldquo;${module.az.primary}&rdquo;
}
```</p>

<p>Note that for this to work, you will need a <em>[demo]</em> section in your ~/.aws/credentials file, as the variables.tf.json file
is <em>not</em> committed to the repository, unlike the last example.</p>

<h3>Lets build some damn infrastructure already!</h3>

<p>We&rsquo;ve now got the pieces we need to put together a VPC, so lets go ahead and <a href="https://github.com/bobtfish/terraform-vpc">write a module to do that</a>.</p>

<p>This takes a /16 network and builds out a VPC with public (internet IP), private (fixed address) and ephemeral (ASG/ELB) subnets
in two availability zones.</p>

<p>Note that we have <em>a lot</em> <a href="https://github.com/bobtfish/terraform-vpc/blob/master/outputs.tf">of outputs here</a>, as we need to expose
every piece of infastructure we want to be able to reference to the module&rsquo;s users.</p>

<h3>But I meant an actual machine!</h3>

<p>That&rsquo;s the next step, we want to launch <a href="https://github.com/bobtfish/terraform-vpc-nat/blob/master/main.tf#L47">our first NAT machine</a>, and again, we&rsquo;ll <a href="https://github.com/bobtfish/terraform-vpc-nat">write a module for it</a>!</p>

<p>We use <a href="https://help.ubuntu.com/community/CloudInit#Cloud_Config_Syntax">cloud-config</a> to setup a firewall on the initial machine, so that subsequent
machines (without public IP addresses) can NAT externally, and we reset the main routing table (used by the &lsquo;back&rsquo; and &lsquo;ephemeral&rsquo; subnets created
by the VPC module) to be one which points to the new NAT instance. For <a href="https://pbs.twimg.com/media/CBHd3OVUwAA8FBh.png:large">good measure</a> we also install <a href="https://www.docker.com/">Docker</a> and <a href="https://puppetlabs.com/">puppet</a>.</p>

<p>Note that we have to use the remote_exec provisioner here to wait for cloud-init to finish, so that we know the firewall rules are in place
for NAT before we launch any more machines</p>

<h2>So when do we get to do something I can actually run?</h2>

<p>Ok, lets put the VPC + nat machine module to work, and <a href="https://github.com/bobtfish/terraform-example-vpc/blob/master/eucentral1-demo/internal.tf">build a host inside the private
subnets</a>.</p>

<p>You can (and should!) fork and clone <a href="https://github.com/bobtfish/terraform-example-vpc">this example</a>, which contains
just enough stuff to string the modules we&rsquo;ve written together and bring up a couple of hosts.</p>

<p>You&rsquo;ll need:</p>

<ul>
<li><a href="https://github.com/bobtfish/terraform">my fork</a> of Terraform</li>
<li><a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-config-files">An ~/.aws/credentials file</a> with a <em>[demo]</em> section</li>
</ul>


<p>Lets go:</p>

<p><code>
~ ⭐ git clone git@github.com:bobtfish/terraform-example-vpc.git
Cloning into 'terraform-example-vpc'...
remote: Counting objects: 61, done.
remote: Compressing objects: 100% (34/34), done.
remote: Total 61 (delta 27), reused 54 (delta 20), pack-reused 0
Receiving objects: 100% (61/61), 15.01 KiB | 0 bytes/s, done.
Resolving deltas: 100% (27/27), done.
Checking connectivity... done.
~ ⭐ cd terraform-example-vpc/
terraform-example-vpc (master👆 ✔) ⭐ make
ssh-keygen -t rsa -f id_rsa -N ''
Generating public/private rsa key pair.
Your identification has been saved in id_rsa.
Your public key has been saved in id_rsa.pub.
The key fingerprint is:3d:c8:0c:76:1e:29:25:a2:52:cd:7d:09:9b:53:24:c9 t0m@somebox
The key's randomart image is:
+--[ RSA 2048]----+
|  .+.o+o+.       |
| . .E..Bo.       |
|. .   B.+        |
| .   . O +       |
|        S o      |
|           o     |
|                 |
|                 |
|                 |
+-----------------+
true
terraform-example-vpc (master👆 ✔) ⭐
</code></p>

<p>This has setup an ssh key to allow you to log into the created instances.</p>

<p>Now, change directory into the region+account folder (<a href="https://github.com/hashicorp/terraform/pull/1281">Terraform can only do one region at once currently</a>),
and run make again. This pulls in all the necessary modules (using terraform get), and then runs their Makefiles by iterating over .terraform/modules to ensure that <em>variables.tf.json</em> is built it needed.</p>

<p>Note that even modules which don&rsquo;t need to build <em>variables.tf.json</em> are required to have a Makefile (which can do nothing).</p>

<p><code>``
eucentral1-demo (master👆 ✔) ⭐ make
terraform get
Get: git::https://github.com/terraform-community-modules/tf_aws_ubuntu_ami.git
Get: git::https://github.com/bobtfish/terraform-vpc-nat.git
Get: git::https://github.com/bobtfish/terraform-vpc.git
Get: git::https://github.com/terraform-community-modules/tf_aws_ubuntu_ami.git
Get: git::https://github.com/bobtfish/terraform-azs.git
for i in $(ls .terraform/modules/); do make -C ".terraform/modules/$i"; done
make[1]: Entering directory</code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/1b2917ba643e4bf90b900b6da59b2e05'
ruby getvariables.rb
make[1]: Leaving directory <code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/1b2917ba643e4bf90b900b6da59b2e05'
make[1]: Entering directory</code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/3f6892e390615d3c9f3cc69d65d7291c'
true
make[1]: Leaving directory <code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/3f6892e390615d3c9f3cc69d65d7291c'
make[1]: Entering directory</code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/be339b8825e57c8cb0372c530eaf49bf'
true
make[1]: Leaving directory <code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/be339b8825e57c8cb0372c530eaf49bf'
make[1]: Entering directory</code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/d4386a9079b869044647990c72b8f0e4'
make[1]: Nothing to be done for <code>all'.
make[1]: Leaving directory</code>/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/d4386a9079b869044647990c72b8f0e4'
ruby getvariables.rb > variables.tf.json
eucentral1-demo (master👆 ✔) ⭐ terraform apply
aws_key_pair.deployer: Creating&hellip;</p>

<p>&hellip; loads more stuff deleted &hellip;</p>

<p>Outputs:</p>

<p>  account                   = demo
  admin_key_name            = deployer-key
  azs                       = eu-central-1a,eu-central-1b
  cidr_block                = 10.1.0.0/16
  dedicatedsubnets          = subnet-8a1ec9e3,subnet-0ea25475
  default_network_acl_id    = acl-d14488b8
  default_security_group_id = sg-16cc1f7f
  ephemeralsubnets          = subnet-8d1ec9e4,subnet-3fa05644
  frontsubnets              = subnet-8c1ec9e5,subnet-0da25476
  id                        = vpc-540dcf3d
  main_route_table_id       = rtb-998d42f0
  nat_instances             = i-91782a5f,i-e7d96d26
  nat_private_ips           = 10.1.0.99,10.1.1.75
  nat_public_ips            = 52.28.54.125,52.28.54.128
  networkprefix             = 10.1
  private-routetable        = rtb-998d42f0
  public-routetable         = rtb-958e41fc
  region                    = eu-central-1
  security_group_allow_all  = sg-edcc1f84
```</p>

<p>You should be able to ssh to your nat instance:</p>

<pre><code>ssh-add ../id_rsa
make sshnat
</code></pre>

<p>and from there, ssh into your back network host:</p>

<pre><code>ssh 10.1.10.4
</code></pre>

<p>and you should see <a href="https://www.consul.io/">consul</a> running (in Docker <a href="https://pbs.twimg.com/media/CBHd3OVUwAA8FBh.png:large">of course</a>)</p>

<p>```
ubuntu@ip-10-1-1-4:~$ ping -c 2 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=56 time=6.74 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=56 time=6.96 ms</p>

<p>&mdash;&ndash; 8.8.8.8 ping statistics &mdash;&ndash;
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 6.746/6.856/6.966/0.110 ms
ubuntu@ip-10-1-1-4:~$ sudo docker ps
CONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS              PORTS                                                      NAMES
d63b2034bf80        fhalim/consul:latest   &ldquo;/bin/sh -c &lsquo;/usr/lo   6 seconds ago       Up 6 seconds        8400/tcp, 0.0.0.0:8500->8500/tcp, 0.0.0.0:8600->8600/udp   consul
```</p>

<h2>Reusing</h2>

<p>As most of the logic is in modules, to create another VPC in another environment or account, the top level directory can just be copied,
the terraform.tfstate file removed and the details in <em>terraform.tfvars</em> updated.</p>

<h2>Conclusion</h2>

<p>Whilst a bunch of stuff in the current demo is more simplistic than a real infrastructure, it shows that Terraform can be used to
bootstrap entire VPCs with non-trivial configurations.</p>

<p>I haven&rsquo;t (yet) installed anything &lsquo;real&rsquo; other than a docker container and some iptables rules as cloud-init is a terrible
config management mechanism! However the current example should be useful as a platform for jumping off from, either
on top of pre-existing infrastructre and puppet masters/chef servers (add vpc peering or vpns), or with ssh based config management
(ansible, salt).</p>

<h2>What&rsquo;s next?</h2>

<p>In the next post (or rather, in my next set of hacking that I may or may not write up ;), I plan to use the infrastructure details of this subnet to test out
the remote module feaure, try to get a puppetmaster (with EC2 tags as the ENC) up, and explore storing the state in <a href="https://www.consul.io/">consul</a>.</p>
]]></content>
  </entry>
  
</feed>
